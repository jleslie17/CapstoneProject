---
title: "Building a tool to assess business performance based on customer reviews"
author: "Jonathan Leslie"
date: "November 22, 2015"
output: 
        pdf_document
---

#Introduction
With the increasing popularity of on-line crowd-sourced review databases such as Yelp, both consumers and business owners have access to a wealth of customer review data that was not possible a decade ago. For the business owner, this data represents a valuable resource, providing insight into customer satisfaction and business performance. While objective assessments such as star ratings and tick-boxes may give some idea as to customer satisfaction, they do little to provide real insight into exactly how a given business is meeting the expectations of its customers, or as the case may be, falling short. 

To fill this gap, customers are encouraged to write reviews that appear as blocks of text. These give the consumer a chance to elaborate on his or her experience with more specific descriptions and they offer a wealth of information for the business owner. Unfortunately, reading customer reviews can be a long, painstaking process. For the business owner wanting to draw general conclusions from hundreds of such documents, the task can be arduous and prohibitively time-consuming. 

This project is a feasibility study aimed at creating an application that can help streamline this process, making it easier for business owners to gain more insight into the satisfaction of their customers from Yelp reviews. By submitting a query by Yelp business ID, the application will return several pieces of information. The first is a plot of average star ratings (averaged monthly) over time, showing the general trend in the business’ customer satisfaction. The second is list of words that are associated with the business’ positive and negative reviews, using text mining algorithms and a user-defined threshold star rating (to distinguish “positive” from “negative” reviews). If, for example, a restaurant owner discovers that the word “wait” is often found in the negative reviews, he or she might infer that customers are often critical of how long they must wait for either seating or food/drink service. The goal of this project is to determine if such text-mining algorithms can be tuned to provide useful information to the business owner.

#Methods and Data
##Loading Data
The data for this study came from the Yelp Dataset Challenge (http://www.yelp.com/dataset_challenge). This dataset includes five JSON files corresponding to several aspect of Yelp review data and should be unzipped in the working directory. For this study, I used only the files pertaining to business IDs and reviews. I loaded the files of interest as below and saved them as rds files for subsequent analyses.
```{r, message=F, echo=F}
library(jsonlite)
library(readr)
library(tm)
library(dplyr)
library(zoo)
library(ggplot2)
require(wordcloud)
```
```{r, eval=FALSE, echo=F}
PATH <- "yelp_dataset_challenge_academic_dataset"
BusData <- stream_in(file(paste(PATH, "/",
                                "yelp_academic_dataset_business.json",sep = "")))
reviewData <- stream_in(file(paste(PATH, "/", 
                                   "yelp_academic_dataset_review.json", sep = "")))
saveRDS(reviewData, "reviewData.rds")
saveRDS(BusData, "businessData.rds")
```

Becuase this is a feasibility study, I used only a subset of the data. In this case, I examined only restaurants and only those restaurants that had 100 or more reviews in the Yelp data. 
```{r, echo=FALSE}
#Reload the data
BusData <- readRDS("businessData.rds")
ReviewData <- readRDS("reviewData.rds")

#Grab all businesses that have "Restaurants" in their category
Restaurants <- BusData[grepl("Restaurants", BusData$categories),]
#Grabbing all reviews of the restaurant businesses
RestRevs <- ReviewData[ReviewData$business_id %in% 
                               Restaurants$business_id,]
#Selecting a shortlist of restaurants that have 50 or more reviews
Threshold <- 100
RestsHiCount <- Restaurants[Restaurants$review_count >= Threshold,]
#Gather all the reviews for those restaurants
RestRevsHiCount <- RestRevs[RestRevs$business_id %in%
                                    RestsHiCount$business_id,]
```

##Examining changes in performance over time
A central tenant of business is to “know your audience”. While a business may have met the demands of its audience well in the past, consumer taste changes quickly, and it can be hard to stay on top of that as a business owner. To assess how well a business is meeting the demands of its public, I wanted to examine ratings over time. To do this, I wrote a script that gathered review data for an example business (using the business_id field of the business data file, assigned to the variable BID (Business ID). This used only the business_id, review_id, date, stars and text variables of the review data file. Reviews were gathered by month using the as.yearmon() command and average monthly star ratings were calculated. The first six lines of the resulting data frame are shown below.

```{r, echo=F}
#Will have BID defined here.
BID <- "2WHP5nhS1rFszfRBKe6fWQ" 

#This is where I put in the business ID of interest
#Here we build a dataframe with the monthly averages
#for stars from the BID
BIDRevs <- RestRevsHiCount[RestRevsHiCount$business_id == BID,]
#Make a new data frame with only stars and date data
BIDStarsDates <- select(BIDRevs, business_id, 
                      date, stars, review_id, text)
#Add a variable 'YearMonth' that is only the month, ignoring days
BIDStarsDates$YearMonth <- strftime(BIDStarsDates$date, "%Y-%m")
BIDStarsDates$YearMonth <- as.Date(as.yearmon(BIDStarsDates$YearMonth))
#Group by business and YearMonth
BIDStarsDatesGrouped <- group_by(BIDStarsDates, business_id, YearMonth)
#Calculate the average stars per month and discard RevID, Date, Text
BIDStarsDatesGrouped <- summarise(BIDStarsDatesGrouped, 
                                 AvStarsPerMonth = mean(stars))
#Convert back into a data frame
BIDStarsDatesGrouped <- data.frame(BIDStarsDatesGrouped)
head(BIDStarsDatesGrouped)
```

To visualise the trend, the program plots average stars over time and draws a linear regression line. The slope of this line can be thought of as the trend in customer satisfaction.
```{r}
fit <- lm(AvStarsPerMonth~YearMonth, data = BIDStarsDatesGrouped)
```
```{r, echo=F, out.width='.49\\textwidth', fig.align='center'}
#Plot the averages over time
Data <- BIDStarsDatesGrouped
g <- ggplot(Data, 
             aes(x = YearMonth, y = AvStarsPerMonth))
p <- g + geom_point() + 
        stat_smooth(method = lm, colour = "blue") +
        ggtitle("Plot 1 \nChanges in star ratings over time") +
        ylim(0,5) +
        xlab("Year") +
        ylab("Average Star rating \n(calculated monthly") +
        theme(axis.title.x=element_text(size = 14)) +
        theme(axis.title.y=element_text(size = 14))
print(p)
```

Change in star rating (stars per year) for business `r BID`:  __`r round(365*fit$coef[[2]], 2)`__

As we can see in the plot, for this example restaurant, the reviews have, on average, gotten worse with time. The owner of this restaurant might want to know why. The next section deals with ways to help answer that question.

##Generating word lists for good and bad reviews
To help shed light on factors that might be contributing to customer satisfaction (either for the good or the bad), I wanted to emply text-mining techniques on the texts that accompany each review. I hypothesize that such an analysis might reveal trends associated with poor customer satisfaction.

Since each business owner may want to decide what star rating differentiates between a "good" and a "bad" review, I allow this threshold to be set. In this case, it is set with the StarsThresh variable. In this example, StarsThresh is set to 3.5, so reviews receiving 1, 2 or 3 stars will be pooled in the "bad" group, and the rest will be considered "good". 

```{r}
StarsThresh <- 3.5
```

To perform text-mining, I used the tm package. The program generated a corpus for all reviews in each pool (good and bad). The corpora were preprocessed to remove whitespace, transform all words to lowercase, remove stopwords (such as 'the') and stemmed to retrieve word radicals (free of suffixes). Punctuation was also removed. Document-term matrices were generated for each corpus, and mean frequency (how often each term appeared in a corpus) was calculated for each term. The terms were then sorted in order of decreasing frequency and the top 20 were printed as output.

```{r, echo=FALSE, comment=''}
QueryDB <- BIDStarsDates
QueryDBGood <- QueryDB[QueryDB$stars > StarsThresh,]
QueryDBBad <- QueryDB[QueryDB$stars <= StarsThresh,]

#Function to make a corpus
GetCorpus <- function(inputDB){
        CorpusTemp <- VCorpus(VectorSource((inputDB$text)))
        CorpusTemp <- tm_map(CorpusTemp, stripWhitespace)
        CorpusTemp <- tm_map(CorpusTemp, content_transformer(tolower))
        CorpusTemp <- tm_map(CorpusTemp, removeWords, stopwords("english"))
        CorpusTemp <- tm_map(CorpusTemp, stemDocument) 
        CorpusTemp <- tm_map(CorpusTemp, removePunctuation)
        return(CorpusTemp)
}

CorpusBad <- GetCorpus(QueryDBBad)
CorpusGood <- GetCorpus(QueryDBGood)

##Function to build DTM
getDTM <- function(inputCorp){
        dtmTemp <- DocumentTermMatrix(inputCorp)
        dtmTemp <- removeSparseTerms(dtmTemp,0.9)
        return(dtmTemp)
}

DTMGood <- getDTM(CorpusGood)
DTMBad <- getDTM(CorpusBad)
KeywordsGood <- sort(round(apply(DTMGood, MARGIN = 2, FUN = mean),2),
                     decreasing = T)
KeywordsBad <- sort(round(apply(DTMBad, MARGIN = 2, FUN = mean),2),
                    decreasing = T)
print("Keywords associated with good reviews:")
print(KeywordsGood[1:20])
print("Keywords associated with bad reviews:")
print(KeywordsBad[1:20])
```

To better visualize word usage, I took advantage of the wordcloud package to produce a wordcloud for each group (good and bad reviews).   

```{r, echo=FALSE, fig.height=3, fig.width=3, dev='png'}
#To follow wordcloud's recipe
mGood <- as.matrix(DTMGood)
mBad <- as.matrix(DTMBad)

vGood <- sort(colSums(mGood), decreasing = T)
vBad <- sort(colSums(mBad), decreasing = T)
dGood <- data.frame(word = names(vGood), freq=vGood)
dBad <- data.frame(word = names(vBad), freq=vBad)

wordcloud(dGood$word, dGood$freq, scale=c(2.5,0.5), 
          max.words=50, random.order=FALSE, 
          rot.per=0.35, use.r.layout=FALSE, 
          colors=brewer.pal(8, 'Dark2'))
wordcloud(dBad$word, dBad$freq, scale=c(2.5,0.5), 
          max.words=50, random.order=FALSE, 
          rot.per=0.35, use.r.layout=FALSE, 
          colors=brewer.pal(8, 'Dark2'))
```


#Results

#Discussion

